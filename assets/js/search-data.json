{
  
    
        "post0": {
            "title": "Title",
            "content": "Sentiment Analysis for IMDb reviews . We will build a classification model to predict whether a movie review from IMDb website is positive or negative using neural networks. We will use the IMDb Dataset of 50K Movie Reviews from Kaggle for this purpose. . You can run this notebook on Google Colab. . import pandas as pd import numpy as np from numpy.random import seed seed(10) # If you are using Google Colab, run the following to upload the dataset # from google.colab import files # uploaded = files.upload() df = pd.read_csv(&quot;IMDB Dataset.csv&quot;, encoding=&#39;utf-8&#39;) df.head() . review sentiment . 0 One of the other reviewers has mentioned that ... | positive | . 1 A wonderful little production. &lt;br /&gt;&lt;br /&gt;The... | positive | . 2 I thought this was a wonderful way to spend ti... | positive | . 3 Basically there&#39;s a family where a little boy ... | negative | . 4 Petter Mattei&#39;s &quot;Love in the Time of Money&quot; is... | positive | . The dataset consists of two columns - review and its sentiment. . Let us check the number of rows and columns in the dataset. . df.shape . (50000, 2) . There are 50,000 movie reviews in total. . Let us check the number of positive and negative reviews in our dataset. . df[&#39;sentiment&#39;].value_counts() . negative 25000 positive 25000 Name: sentiment, dtype: int64 . The dataset is equally divided into positive and negative reviews. . Steps for building a sentiment detection model: . Preprocessing text Cleaning the text | Splitting the dataset into training and validation sets | Converting the reviews into numerical vectors (vectorization) | . | Training a neural network for sentiment detection Defining the architecture of the neural net | Training on the dataset | Computing accuracy scores on training and validation sets | . | Testing the model to detect sentiment for new unseen reviews | . Let us peek into one of the movie review. . text = df.iloc[4, 0] text . &#39;Petter Mattei &#39;s &#34;Love in the Time of Money&#34; is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler &#39;s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.&lt;br /&gt;&lt;br /&gt;The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.&lt;br /&gt;&lt;br /&gt;The acting is good under Mr. Mattei &#39;s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.&lt;br /&gt;&lt;br /&gt;We wish Mr. Mattei good luck and await anxiously for his next work.&#39; . The text contains HTML tags such as &lt;br /&gt; that we will remove using regular expressions. . import re text = re.sub(&#39;&lt;.*?&gt;&#39;, &#39;&#39;, text) text . &#39;Petter Mattei &#39;s &#34;Love in the Time of Money&#34; is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. This being a variation on the Arthur Schnitzler &#39;s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.The acting is good under Mr. Mattei &#39;s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.We wish Mr. Mattei good luck and await anxiously for his next work.&#39; . We define a clean_text function that we will use below while vectorizing the text. . import re def clean_text(text): text = re.sub(r&#39;&lt;.*?&gt;&#39;, &#39;&#39;, text) # remove HTML tags return text . Now we extract our input feature X and the target output y from the dataframe df. . X = df[&#39;review&#39;] y = df[&#39;sentiment&#39;].replace({&#39;positive&#39;: 1, &#39;negative&#39;: 0}) . Then we split the dataset into train and validation sets. . from sklearn.model_selection import train_test_split # default is 75% / 25% train-test split X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0) . Vectorization . Our input X consists of texts, which are sequences of words, spaces, punctuations, emojis, etc. and we need to convert these texts into a numerical vectors. There are multiple ways to vectorize the text and we will use TF-IDF vectorization with Bag-Of-Words (BOW) method. . Bag-Of-Words (BOW) method simply means we count the occurence of words without keeping track of the order of words in a sentence. The steps for vectorizing using TF-IDF are as follows: . We first create a vocabulary of words and compute the size of the vocabulary, say $n$. | For $n$-dimensional vector, we assign each index to a specific word in the vocabulary. | Next we iterate through the text samples in the dataset: we compute TF-IDF frequency for each word and | we create a vector by assigning the above frequencies in their respective indices and use zeros for the words not present in the sample text. | . | . Term Frequency Inverse Document Frequency (TF-IDF) . TF-IDF frequency tweaks the usual frequency so that rarer words are given more weight. Some words such as &quot;wonderful&quot;, &quot;disgusting&quot;, etc. would be stronger indicators for the sentiment of the reviews than words such as &quot;watching&quot;, &quot;become&quot;, &quot;every&quot;, &quot;after&quot;, etc. and hence, it makes sense to use TF-IDF for our project. . $$ text{TF-IDF} = text{TF (Term Frequency)} * text{IDF (Inverse Document Frequency)} $$ . Term Frequency (TF) is the number of times a word occur in a review. It is multiplied by Inverse Document Frequency (IDF) which is a measure of the originality of the word. The words that are rarer have higher IDF values and hence, they are weighted more in TF-IDF than their true frequency as compared to commonly occuring words. . $$ text{Inverse Document Frequency (IDF) for a word} = log Bigg( frac{ text{Total number of reviews}}{ text{Number of reviews that contain this word}} Bigg)$$ . from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer(stop_words=&quot;english&quot;, preprocessor=clean_text, # clean_text function defined above max_features=15000) X_train_vectorized = vectorizer.fit_transform(X_train).toarray() X_valid_vectorized = vectorizer.transform(X_valid).toarray() . Constructing a simple network . We import the relevant functions from Keras that we plan to use in our network. . from keras.models import Sequential from keras.layers import Embedding, Dense, Dropout from keras.initializers import he_normal from keras.callbacks import ModelCheckpoint, EarlyStopping input_dim = X_train_vectorized.shape[1] # size of input variables . Let us check the shapes of the training and validation sets. . X_train.shape, X_valid.shape, y_train.shape, y_valid.shape . ((37500,), (12500,), (37500,), (12500,)) . Let us define a simple neural network with a single hidden layer. We use ReLU for the hidden layer and sigmoid activation for the output layer since it is a binary classification problem. . model = Sequential() model.add(Dense(units=32, input_dim=input_dim, activation=&quot;relu&quot;, kernel_initializer=he_normal())) # Hidden layer model.add(Dropout(0.4)) # Adding dropout regularization to the hidden layer with dropout prob=0.5 model.add(Dense(units=1, activation=&quot;sigmoid&quot;)) # Output layer model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 32) 480032 _________________________________________________________________ dropout (Dropout) (None, 32) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 33 ================================================================= Total params: 480,065 Trainable params: 480,065 Non-trainable params: 0 _________________________________________________________________ . We compile the model by using accuracy as the evaluation metric, ADAM as optimizer and binary cross-entropy loss funtion. . model.compile(optimizer=&#39;adam&#39;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . Finally, we train the model while using EarlyStopping for the training to stop early if the validation loss is not decreasing any further. . earlystopping = EarlyStopping(monitor=&#39;val_loss&#39;, mode=&#39;min&#39;, verbose=1) model_history = model.fit(X_train_vectorized, y_train, epochs=10, batch_size=256, validation_data=(X_valid_vectorized, y_valid), verbose=1, callbacks=earlystopping); . Epoch 1/10 147/147 [==============================] - 3s 20ms/step - loss: 0.6252 - accuracy: 0.7680 - val_loss: 0.4148 - val_accuracy: 0.8698 Epoch 2/10 147/147 [==============================] - 1s 8ms/step - loss: 0.3653 - accuracy: 0.8857 - val_loss: 0.3021 - val_accuracy: 0.8879 Epoch 3/10 147/147 [==============================] - 1s 8ms/step - loss: 0.2618 - accuracy: 0.9127 - val_loss: 0.2671 - val_accuracy: 0.8934 Epoch 4/10 147/147 [==============================] - 1s 9ms/step - loss: 0.2166 - accuracy: 0.9257 - val_loss: 0.2540 - val_accuracy: 0.8942 Epoch 5/10 147/147 [==============================] - 1s 10ms/step - loss: 0.1836 - accuracy: 0.9380 - val_loss: 0.2512 - val_accuracy: 0.8941 Epoch 6/10 147/147 [==============================] - 1s 8ms/step - loss: 0.1663 - accuracy: 0.9444 - val_loss: 0.2536 - val_accuracy: 0.8922 Epoch 00006: early stopping . We can plot the loss function values for training and validation sets over the epochs using model_history. . import matplotlib.pyplot as plt def plot_loss(model_history): fig, ax = plt.subplots() plt.plot(np.sqrt(model_history.history[&#39;loss&#39;]), &#39;r&#39;, label=&#39;train&#39;) plt.plot(np.sqrt(model_history.history[&#39;val_loss&#39;]), &#39;b&#39; ,label=&#39;val&#39;) ax.set_xlabel(r&#39;Epoch&#39;, fontsize=16) ax.set_ylabel(r&#39;Loss&#39;, fontsize=16) plt.legend() plt.title(&quot;Loss over the epochs&quot;, fontsize=20) fig.tight_layout() plt.show() plot_loss(model_history) . We can also plot the accuracy values for training and validation sets over the epochs using model_history. . def plot_accuracy(model_history): fig, ax = plt.subplots() plt.plot(np.sqrt(model_history.history[&#39;accuracy&#39;]), &#39;r&#39;, label=&#39;train&#39;) plt.plot(np.sqrt(model_history.history[&#39;val_accuracy&#39;]), &#39;b&#39; ,label=&#39;val&#39;) ax.set_xlabel(r&#39;Epoch&#39;, fontsize=16) ax.set_ylabel(r&#39;Accuracy&#39;, fontsize=16) plt.legend() plt.title(&quot;Accuracy over the epochs&quot;, fontsize=20) fig.tight_layout() plt.show() plot_accuracy(model_history) . Let us use our trained neural network to predict the sentiment for the movie reviews. Note that the network is giving us the probability that a review is positive, and thus we expect higher probabilities for positive reviews whereas lower probabilities for negative ones. . Let us check with a straightforward positive review. . review1 = [&quot;What a gripping, heart wrenching, hard hitting film...&quot;] vectorized_review1 = vectorizer.transform(review1).toarray() model.predict(vectorized_review1) . array([[0.97009695]], dtype=float32) . The model classified the above review correctly as positive with a high probability. Let us now check for a negative review. . review2 = [&quot;That&#39;s nothing new in this film.&quot;] vectorized_review2 = vectorizer.transform(review2).toarray() model.predict(vectorized_review2) . array([[0.0942305]], dtype=float32) . The model again classified the above negative review correctly by giving a low probability score. . Our model is looking at individual words weighted by TF-IDF frequency but it does not take into account the sequence of the words, and thus missing the context. Let us see how it does with more reviews. . review2 = [&quot;If you can keep both eyes open through its whole three-hour length you&#39;re a better man than I am.&quot;] vectorized_review2 = vectorizer.transform(review2).toarray() model.predict(vectorized_review2) . array([[0.69224685]], dtype=float32) . It is getting the prediction wrong by a long margin, probably because most words are neutral except for better which is usually positive. Let us check with another review where sequence of the words matter a lot to get a sense of the sentiment. . review2 = [&quot;In and of itself it is not a bad film.&quot;] vectorized_review2 = vectorizer.transform(review2).toarray() model.predict(vectorized_review2) . array([[0.00568324]], dtype=float32) . The model got it wrong again. Let us check it with another similar review. . review3 = [&quot;This show was not really funny anymore.&quot;] vectorized_review3 = vectorizer.transform(review3).toarray() model.predict(vectorized_review3) . array([[0.8506278]], dtype=float32) . Same result as above. . Though we are able to achieve an accuracy score of $~90 %$, the above examples illustrates that the neural network architectures that makes use of the sequence of words in a sentiment might be a better fit for this project. .",
            "url": "https://aashitak.github.io/CS121ProjectExample1/2022/07/10/Sentiment-Analysis-for-IMDb-reviews-CS121.html",
            "relUrl": "/2022/07/10/Sentiment-Analysis-for-IMDb-reviews-CS121.html",
            "date": " • Jul 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://aashitak.github.io/CS121ProjectExample1/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://aashitak.github.io/CS121ProjectExample1/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://aashitak.github.io/CS121ProjectExample1/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://aashitak.github.io/CS121ProjectExample1/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}